% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  man]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdflang={en-EN},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{Predicting Voter Turnout with Machine Learning}
\keywords{keywords\newline\indent Word count: X}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\author{\phantom{0}}
\date{}


\affiliation{\phantom{0}}

\begin{document}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.06}}@{}}
\toprule
\endhead
title : ``ML and Voter Turnout''
shorttitle : ``Predicting Voter Turnout with Machine Learning'' \\
author:
- name : ``Sean H. Merritt''
affiliation : ``1''
corresponding : yes \# Define only one corresponding author
email : ``\href{mailto:sean.merritt@cgu.edu}{\nolinkurl{sean.merritt@cgu.edu}}''
role: \# Contributorship roles (e.g., CRediT, \url{https://casrai.org/credit/})
- Conceptualization
- Writing - Original Draft Preparation
- Writing - Review \& Editing
- name : ``Carlos Algara''
affiliation : ``1,2''
role:
- Writing - Review \& Editing \\
affiliation:
- id : ``1''
institution : ``Claremont Graduate University'' \\
authornote: \textbar{}
Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line. \\
Enter author note here. \\
abstract: \textbar{} \\
keywords : ``keywords''
wordcount : ``X'' \\
bibliography : {[}``r-references.bib''{]} \\
floatsintext : no
figurelist : no
tablelist : no
footnotelist : no
linenumbers : yes
mask : no
draft : no \\
documentclass : ``apa6''
classoption : ``man''
output : papaja::apa6\_pdf \\
\bottomrule
\end{longtable}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

\hypertarget{material}{%
\subsection{Material}\label{material}}

\hypertarget{procedure}{%
\subsection{Procedure+}\label{procedure}}

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

We used R {[}Version 4.0.4; (\textbf{R-base?}){]} and the R-package \emph{papaja} {[}Version 0.1.0.9997; (\textbf{R-papaja?}){]} for all our analyses.

\hypertarget{results}{%
\section{Results}\label{results}}

\begin{verbatim}
## [09:57:35] WARNING: ..\src\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
##               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,
##               importance_type='gain', interaction_constraints='',
##               learning_rate=0.01, max_delta_step=0, max_depth=6,
##               min_child_weight=4, missing=nan, monotone_constraints='()',
##               n_estimators=2000, n_jobs=4, nthread=4, num_parallel_tree=1,
##               random_state=27, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1,
##               seed=27, subsample=0.9, tree_method='exact',
##               validate_parameters=1, verbosity=None)
## 
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
##   warnings.warn(label_encoder_deprecation_msg, UserWarning)
\end{verbatim}

\begin{verbatim}
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption
##   "memory consumption")
\end{verbatim}

\begin{verbatim}
## [09:59:47] WARNING: ..\src\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
##               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,
##               importance_type='gain', interaction_constraints='',
##               learning_rate=0.01, max_delta_step=0, max_depth=6,
##               min_child_weight=4, missing=nan, monotone_constraints='()',
##               n_estimators=2000, n_jobs=4, nthread=4, num_parallel_tree=1,
##               random_state=27, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1,
##               seed=27, subsample=0.9, tree_method='exact',
##               validate_parameters=1, verbosity=None)
## 
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
##   warnings.warn(label_encoder_deprecation_msg, UserWarning)
\end{verbatim}

\begin{verbatim}
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption
##   "memory consumption")
\end{verbatim}

\begin{verbatim}
## [10:00:26] WARNING: ..\src\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
##               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,
##               importance_type='gain', interaction_constraints='',
##               learning_rate=0.01, max_delta_step=0, max_depth=6,
##               min_child_weight=4, missing=nan, monotone_constraints='()',
##               n_estimators=2000, n_jobs=4, nthread=4, num_parallel_tree=1,
##               random_state=27, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1,
##               seed=27, subsample=0.9, tree_method='exact',
##               validate_parameters=1, verbosity=None)
## 
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
##   warnings.warn(label_encoder_deprecation_msg, UserWarning)
\end{verbatim}

\begin{verbatim}
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption
##   "memory consumption")
\end{verbatim}

\begin{verbatim}
## [10:01:06] WARNING: ..\src\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
##               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,
##               importance_type='gain', interaction_constraints='',
##               learning_rate=0.01, max_delta_step=0, max_depth=6,
##               min_child_weight=4, missing=nan, monotone_constraints='()',
##               n_estimators=2000, n_jobs=4, nthread=4, num_parallel_tree=1,
##               random_state=27, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1,
##               seed=27, subsample=0.9, tree_method='exact',
##               validate_parameters=1, verbosity=None)
## 
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].
##   warnings.warn(label_encoder_deprecation_msg, UserWarning)
\end{verbatim}

\begin{verbatim}
## C:\Users\seanm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\xgboost\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption
##   "memory consumption")
\end{verbatim}

\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tab:unnamed-chunk-9}Yearly Accuracy}\tabularnewline
\toprule
Partison & Accuracy & AUC \\
\midrule
\endfirsthead
\toprule
Partison & Accuracy & AUC \\
\midrule
\endhead
In-party & 0.6861629 & 0.7233868 \\
Non-party & 0.6621859 & 0.7157141 \\
Out-party & 0.6917731 & 0.7159107 \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}rrr@{}}
\caption{\label{tab:unnamed-chunk-12}Yearly Accuracy}\tabularnewline
\toprule
Year & Accuracy & AUC \\
\midrule
\endfirsthead
\toprule
Year & Accuracy & AUC \\
\midrule
\endhead
2006 & 0.5663643 & 0.5788361 \\
2008 & 0.7031381 & 0.6345702 \\
2010 & 0.7096642 & 0.7178568 \\
2012 & 0.7100576 & 0.6836987 \\
2014 & 0.6752250 & 0.7374063 \\
2016 & 0.6535725 & 0.6890358 \\
2018 & 0.6848255 & 0.7273221 \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}rlrr@{}}
\caption{\label{tab:unnamed-chunk-12}Yearly Accuracy}\tabularnewline
\toprule
Year & Partison & Accuracy & AUC \\
\midrule
\endfirsthead
\toprule
Year & Partison & Accuracy & AUC \\
\midrule
\endhead
2006 & In-Party & 0.5517393 & 0.5618903 \\
2006 & Non-party & 0.5855750 & 0.5773263 \\
2006 & Out-party & 0.5532189 & 0.5654725 \\
2008 & In-Party & 0.7591301 & 0.5962034 \\
2008 & Non-party & 0.6412178 & 0.6039731 \\
2008 & Out-party & 0.7156213 & 0.6062223 \\
2010 & In-Party & 0.6822558 & 0.7148535 \\
2010 & Non-party & 0.6860158 & 0.6925272 \\
2010 & Out-party & 0.7353516 & 0.6752830 \\
2012 & In-Party & 0.6823038 & 0.6511388 \\
2012 & Non-party & 0.6727465 & 0.6770060 \\
2012 & Out-party & 0.7503303 & 0.6295781 \\
2014 & In-Party & 0.6779009 & 0.7426735 \\
2014 & Non-party & 0.6878049 & 0.7411303 \\
2014 & Out-party & 0.6727079 & 0.7200449 \\
2016 & In-Party & 0.6579970 & 0.6711074 \\
2016 & Non-party & 0.6160310 & 0.6606516 \\
2016 & Out-party & 0.6738104 & 0.7113751 \\
2018 & In-Party & 0.6889401 & 0.7106554 \\
2018 & Non-party & 0.6714049 & 0.7187964 \\
2018 & Out-party & 0.6776930 & 0.7056618 \\
\bottomrule
\end{longtable}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\endgroup


\end{document}
